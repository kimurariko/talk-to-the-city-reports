{"pageProps":{"result":{"clusters":[{"cluster":"地元商品とお土産の多様性","cluster_id":"1","takeaways":"「ぽんしゅ館 新潟驛店」は新潟の名産品を一堂に集めたお土産店で、日本酒、地元食材を使った食品、伝統的な調味料、地酒を使った限定お菓子などが揃っています。また、「ぽんしゅ館 コンプレックス」では、新潟の厳選されたお土産からローカル商品まで、そして「ぽんしゅ館 クラフトマンシップ」では金物の町・燕三条の包丁やカトラリーなどを取り扱っています。購入した商品は発送可能で、旅行者は手ぶらで観光を楽しむことができます。","arguments":[{"arg_id":"A1_0","argument":"新潟駅にある「ぽんしゅ館 新潟驛店」は、新潟の名産品が一堂に会する人気のお土産店で、新潟越後の魅力を楽しむことができる。","comment_id":"1","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.503905,"y":18.238546,"p":1},{"arg_id":"A4_0","argument":"ぽんしゅ館では、日本酒、地元の食材を使った食品、干物、伝統的な調味料、蕎麦、地酒を使った限定お菓子など、多くのお土産が揃っている。購入した商品は発送可能なので、買い物後も手ぶらで観光を楽しむことができる。","comment_id":"4","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.903729,"y":17.769558,"p":0.7512102782255842},{"arg_id":"A14_0","argument":"「ぽんしゅ館 コンプレックス」は、新潟の厳選されたお土産、米菓やお米などの定番商品からローカル商品まで取り揃えている。","comment_id":"14","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.1378927,"y":18.10277,"p":0.6567150437956064},{"arg_id":"A22_0","argument":"「ぽんしゅ館 クラフトマンシップ」では金物の町・燕三条の包丁、カトラリーなどを取り扱っている","comment_id":"22","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.756884,"y":18.37084,"p":1},{"arg_id":"A24_0","argument":"木村さんがお土産をゲットし、ぽんしゅ館で過ごす時間が旅の思い出を深める。","comment_id":"24","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.6715093,"y":17.982601,"p":0.9708053147896674},{"arg_id":"A38_0","argument":"ぽんしゅ館の2つの異なるロゴがデザインされたオリジナルの升","comment_id":"38","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.8636093,"y":18.566296,"p":0.6286933884313942}]},{"cluster":"地元商品の豊富な選択肢","cluster_id":"8","takeaways":"参加者は、西口連絡通路にある3つの店舗が幅広い品揃えを提供していることを強調しました。これらの店舗では、日本酒やおつまみ、地元の工芸品、限定商品などが購入でき、利き酒師や清酒達人検定合格者のスタッフが商品選びのアドバイスを提供しています。これらの店舗は、お土産選びにも便利で、お気に入りの一品を探すのがおすすめとの意見が述べられました。","arguments":[{"arg_id":"A2_0","argument":"店舗は西口連絡通路に3つあり、幅広い品揃えがある。日本酒検定の級を持つ第11代にいがた親善大使の木村さんと一緒にご紹介します。","comment_id":"2","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.5545626,"y":17.003807,"p":0.9690020722069872},{"arg_id":"A13_0","argument":"唎酒番所で提供される日本酒やおつまみのみそは、館内で購入可能で、お土産選びにも便利です。","comment_id":"13","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.8024855,"y":16.806818,"p":1},{"arg_id":"A23_0","argument":"県内で作られたぐい呑みやタンブラーなどの地域の工芸品が揃っており、限定商品もあるため、お気に入りの一品を探すのがおすすめです。","comment_id":"23","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.8298016,"y":17.206335,"p":1},{"arg_id":"A37_0","argument":"売店では越後魚沼の食品や酒器などの商品が豊富に揃っており、スタッフは利き酒師や清酒達人検定合格者なので、商品選びで困ったら気軽に相談してほしい。","comment_id":"37","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.061357,"y":17.025402,"p":1}]},{"cluster":"地元酒試飲体験","cluster_id":"2","takeaways":"参加者は、「唎酒番所 ぽんしゅ館 新潟驛店」が新潟市及び新潟全酒蔵の代表銘柄を試飲できる場所であると述べました。また、地元の酒蔵からの日本酒、ビール、ワインが並び、中には新潟でしか取り扱っていない珍しい銘柄もあると指摘しました。利き酒番所は20歳未満の入場が禁止された大人専用のスペースで、酒売り場の奥に位置しているとのことです。","arguments":[{"arg_id":"A3_0","argument":"館内には地元の酒蔵からの日本酒、ビール、ワインが並び、中には新潟でしか取り扱っていない珍しい銘柄もある。","comment_id":"3","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.4855723,"y":16.475777,"p":1},{"arg_id":"A5_0","argument":"「唎酒番所 ぽんしゅ館 新潟驛店」では新潟市及び新潟全酒蔵の代表銘柄を試飲できる。","comment_id":"5","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":1,"x":5.4014387,"y":16.220152,"p":1},{"arg_id":"A9_0","argument":"北雪酒造の純米酒 北雪を試飲した。","comment_id":"9","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.7909217,"y":15.84366,"p":1},{"arg_id":"A35_0","argument":"利き酒番所は20歳未満の入場が禁止された大人専用のスペースで、酒売り場の奥に位置している。","comment_id":"35","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.956116,"y":16.402622,"p":1},{"arg_id":"A45_0","argument":"新潟の全蔵元の試飲が可能です。","comment_id":"45","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.0315614,"y":15.989642,"p":1}]},{"cluster":"日本酒の試飲体験","cluster_id":"6","takeaways":"参加者は、「角打ち」の利便性と楽しさを強調しました。多数の唎き酒マシーンがあり、新潟の旬のおつまみと日本酒を気軽に楽しむことができると述べています。また、飲み比べることで味の違いを楽しむことができ、冬限定のお燗マシンも利用可能であるとの意見がありました。","arguments":[{"arg_id":"A6_0","argument":"唎き酒マシーンがたくさんある","comment_id":"6","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.3601813,"y":16.085163,"p":0.968718351084449},{"arg_id":"A15_0","argument":"「角打ち」は飲食スペースもあり、新潟の旬のおつまみと日本酒を気軽に楽しむことができる。","comment_id":"15","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.203807,"y":17.242325,"p":0.8417087692137118},{"arg_id":"A42_0","argument":"飲み比べることで味の違いを楽しむことができる。","comment_id":"42","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.5629745,"y":15.810247,"p":0},{"arg_id":"A44_0","argument":"冬限定のお燗マシンが登場し、お湯の入ったおちょこに数秒浸けるだけで使用可能","comment_id":"44","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.177329,"y":16.460945,"p":0}]},{"cluster":"500円での利き酒体験","cluster_id":"0","takeaways":"参加者は、500円で5枚のコインとお猪口を手に入れ、それを使って5種類の飲み物を試すことができる飲酒スポットについて述べています。また、これらのコインはおつまみの交換にも使用可能で、簡単な操作で好きなお酒が手に入るとの意見がありました。","arguments":[{"arg_id":"A7_0","argument":"500円でお猪口とメダル5枚をもらった","comment_id":"7","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.0701463,"y":14.595279,"p":1},{"arg_id":"A12_0","argument":"コインは、おつまみの交換にも使用でき、例えば丸しぼりみそ赤は100円またはコイン1枚で交換可能である。","comment_id":"12","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.487723,"y":14.1641445,"p":0.8251938460010242},{"arg_id":"A25_0","argument":"500円で最大5杯の利酒を楽しめる人気の飲酒スポットがある。","comment_id":"25","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.6435277,"y":15.07588,"p":0},{"arg_id":"A32_0","argument":"500円で5枚のコインを購入し、それを使って5種類の飲み物を試すことができる","comment_id":"32","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":1,"x":3.3907495,"y":14.744926,"p":0.922796408483974},{"arg_id":"A36_0","argument":"コインを入れてボタンを押すだけで、好きなお酒が手に入る","comment_id":"36","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.83791,"y":14.458781,"p":0.746539166075705},{"arg_id":"A43_0","argument":"受付で500円を払って5枚のコインとおちょこを受け取り、1枚のコインで1銘柄のお酒が提供される。","comment_id":"43","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.3466592,"y":14.338065,"p":1},{"arg_id":"A48_0","argument":"500円で約5杯分のおちょこが手に入る","comment_id":"48","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.0023544,"y":14.927042,"p":0.8803567986746184}]},{"cluster":"日本酒の試飲体験","cluster_id":"7","takeaways":"参加者は新潟県内の全88の酒蔵から集められた111の銘柄の日本酒を提供する「唎酒マシン」の設置を提案しました。これにより、季節によって変わる銘柄や、ぽんしゅ館でしか飲めない希少なお酒を試すことができます。これは、県内全蔵の利き酒ができる酒のテーマパークの一部となる可能性があります。","arguments":[{"arg_id":"A8_0","argument":"新潟県内88の酒蔵から集められた111の銘柄の日本酒を選び、コインを入れてボタンを押すと1杯分が注がれる。季節によって銘柄が変わり、ぽんしゅ館でしか飲めない希少なお酒もある。","comment_id":"8","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.966083,"y":15.375832,"p":0},{"arg_id":"A28_0","argument":"唎酒で全蔵制覇を目指す","comment_id":"28","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.1230764,"y":15.20073,"p":1},{"arg_id":"A29_0","argument":"県内全蔵の111種の地酒を飲み比べられる「唎酒マシン」があり、希少種も飲むことができる。","comment_id":"29","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.2976537,"y":15.382392,"p":1},{"arg_id":"A31_0","argument":"県内全蔵の利き酒ができる酒のテーマパークの提案","comment_id":"31","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.437293,"y":15.736217,"p":1}]},{"cluster":"\"爆弾おにぎりと地元のおつまみ\"","cluster_id":"4","takeaways":"参加者は、「爆弾おにぎり家」の南魚沼産コシヒカリを使ったおにぎりを絶賛しました。その大きさ、海苔の量、具材の豊富さ、そして羽釜で炊いた柔らかいお米の美味しさについて特に言及しています。また、新潟のおつまみの品揃えの豊富さも評価しています。","arguments":[{"arg_id":"A10_0","argument":"甘くて美味しい！","comment_id":"10","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.4784555,"y":17.248566,"p":1},{"arg_id":"A16_0","argument":"「爆弾おにぎり家」の南魚沼産コシヒカリを使ったおにぎりは絶品である。","comment_id":"16","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.304618,"y":17.703594,"p":1},{"arg_id":"A18_0","argument":"大きな海苔を4枚つなげて巻き、羽釜で炊いた柔らかいお米とたっぷりの具材を詰めたおにぎりが完成する。","comment_id":"18","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":2.8558502,"y":17.669048,"p":0.9450438205113608},{"arg_id":"A20_0","argument":"パクッ！","comment_id":"20","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.2969184,"y":17.180006,"p":1},{"arg_id":"A27_0","argument":"新潟のおつまみの品揃えが豊富である","comment_id":"27","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.9113896,"y":17.772501,"p":0.8642535888885009}]},{"cluster":"爆弾おにぎり体験","cluster_id":"9","takeaways":"参加者は、飲食店での選択について話し合いました。迷った時は店員に好みを伝えることや、メニューの紹介コメントを参考にすることを提案しています。また、特に「爆弾おにぎり」のような大きな食事に挑戦することを推奨し、食べきれない場合でも持ち帰りが可能であることを指摘しています。さらに、100種類以上の選択肢からお気に入りを見つけることを勧めています。","arguments":[{"arg_id":"A11_0","argument":"迷った時は辛さの度数や紹介コメントを参考にし、店員に好みの味について尋ねるとオススメを教えてもらえる。","comment_id":"11","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.4447424,"y":16.427504,"p":1},{"arg_id":"A17_0","argument":"名物の「爆弾おにぎり」を注文し、その中から5種類の具材を選ぶことができる。","comment_id":"17","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":2.7729452,"y":17.11677,"p":1},{"arg_id":"A19_0","argument":"私の顔ぐらいの大きさの食べ物があり、食べきれるかどうか不安","comment_id":"19","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":2.956697,"y":16.29058,"p":1},{"arg_id":"A21_0","argument":"全てを食べきることはできなかったが、持ち帰りも可能なので試してみることを推奨する。","comment_id":"21","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":3.2027342,"y":15.985836,"p":1},{"arg_id":"A33_0","argument":"100種類以上の選択肢からお気に入りを見つけてみてください。","comment_id":"33","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":2.6167984,"y":16.614754,"p":1}]},{"cluster":"日本酒の試飲体験","cluster_id":"3","takeaways":"参加者は、お酒の購入と楽しみ方について多様な意見を述べました。エキナカでの利き酒体験、越後の酒蔵の銘柄を試す機会、自宅での前掛けの使用、そして全蔵元の味を試せる利き酒コーナーの人気について触れています。これらは、お酒の楽しみ方としての多様性と、個々の好みに合わせた選択肢の重要性を示しています。","arguments":[{"arg_id":"A26_0","argument":"お気に入りのお酒を見つけたら、お酒売り場で購入できる","comment_id":"26","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.353909,"y":14.805538,"p":1},{"arg_id":"A30_0","argument":"エキナカで利き酒が楽しめる","comment_id":"30","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.509008,"y":15.208825,"p":1},{"arg_id":"A34_0","argument":"越後の酒蔵の銘柄が並ぶ利き酒番所","comment_id":"34","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.259912,"y":14.816591,"p":0.8436086835251114},{"arg_id":"A39_0","argument":"自宅で好きな銘柄の前掛けを使用する","comment_id":"39","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.7102027,"y":14.576346,"p":0},{"arg_id":"A47_0","argument":"全蔵元の味を試し飲みできる利き酒コーナーが人気である。","comment_id":"47","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":4.6424775,"y":15.412039,"p":1}]},{"cluster":"日本酒試飲とお土産選びの体験","cluster_id":"5","takeaways":"新潟の酒蔵代表銘柄が試飲できる唎き酒コーナーは、500円で5枚の専用コインを購入して利用でき、非常に人気があります。また、新潟県内の約90蔵の代表的な銘柄から梅酒まで117種類の地酒を楽しむことができるテーマパークとしても機能しています。初心者でも気軽に楽しむことができ、県内外から多くの客が訪れています。\n\nさらに、新潟県全域から集められたお土産、特に地酒を扱う店もあり、アクセスも便利なJR新幹線校内に位置しています。訪問者は自分の好みに合わせて酒を選べることや、新たな発見があることを楽しんでいます。また、お土産選びも番号順の陳列棚があり、選びやすいと評価されています。","arguments":[{"arg_id":"A40_0","argument":"新潟の全酒蔵の代表銘柄が試飲できる唎き酒コーナーが人気で、500円で5枚の専用コインを購入して利用できます。また、店内ではお土産も購入可能で、全ての店舗はJR新幹線校内に位置しているためアクセスが便利です。","comment_id":"40","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":1,"x":6.148112,"y":15.75996,"p":1},{"arg_id":"A41_0","argument":"新潟県内の約90蔵の代表的な銘柄から梅酒まで117種類の地酒を楽しむことができるテーマパーク。初心者でも気軽に楽しめ、県内外から多くの客が訪れている。","comment_id":"41","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":1,"x":5.884654,"y":16.03131,"p":1},{"arg_id":"A46_0","argument":"新潟県全域から集められたお土産、特に地酒を扱う店","comment_id":"46","categoryLabel":1,"kutikomi_unique":0,"koushiki_unique":0,"x":5.784115,"y":16.753841,"p":1},{"arg_id":"A49_0","argument":"新潟に行く理由の一つは、自分の好みに合わせて酒を選べる場所があるからです。好きではないタイプの酒に挑戦すると、新たな発見があり楽しいです。同僚にもこの場所を教え、喜んでもらいました。日本酒が苦手な人でも、新たな発見があるかもしれません。","comment_id":"49","categoryLabel":0,"kutikomi_unique":0,"koushiki_unique":0,"x":6.1612234,"y":16.318197,"p":1},{"arg_id":"A50_0","argument":"新潟のお酒の豊富なラインナップに感銘を受け、ビール党の私が2日で2回訪れた。お土産選びも番号順の陳列棚があり、選びやすかった。","comment_id":"50","categoryLabel":0,"kutikomi_unique":0,"koushiki_unique":0,"x":6.1947722,"y":16.72296,"p":1}]}],"comments":{"1":{"comment":"新潟の銘品が一同に揃う「ぽんしゅ館 新潟驛店」。2013年、新潟の玄関口である新潟駅にオープンして以来、ディープな新潟越後の魅力を楽しめると人気のおみやげ屋さんです。"},"2":{"comment":"幅広い品揃えから店舗は西口連絡通路に全部で3つ。今回は日本酒検定の級を持つ、第１１代にいがた観光親善大使の木村さんとご紹介します。"},"3":{"comment":"館内には県内にある酒蔵の日本酒やビール、ワインなどが並びます。木村さんは地元三条市の「五十嵐川」を発見！中には新潟でしか取り扱っていない珍しい銘柄もありました。"},"4":{"comment":"日本酒の他にも日本海産の素材を使った食品や干物、伝統調味料、蕎麦、ぽんしゅ館限定の地酒を使ったお菓子など、お土産にピッタリな商品がたくさん。館内で購入した商品は発送できるので、買い物後も手ぶらで観光を楽しめるのは嬉しいポイントです。"},"5":{"comment":"「唎酒番所 ぽんしゅ館 新潟驛店」では新潟市はもちろん、新潟全酒蔵の代表銘柄を唎き酒できます。"},"6":{"comment":"唎き酒マシーンがズラリ。"},"7":{"comment":"500円でお猪口とメダル5枚をもらったら、"},"8":{"comment":"好みの銘柄を選びお猪口をセット。コインを入れて、ボタンを押すと1杯分の日本酒がお猪口に注がれます。新潟県内88の酒蔵（2021年8月3日現在）から集められた111の銘柄は季節によって変化。ぽんしゅ館でしか飲めない希少なお酒も要チェックです！"},"9":{"comment":"気になっていた北雪酒造の純米酒 北雪を一口。"},"10":{"comment":"「甘くて美味しい！」"},"11":{"comment":"迷った時は辛さの度数や紹介コメントを参考に。店員さんに質問すれば好みの味からオススメを教えてもらえますよ。"},"12":{"comment":"コインは、みそやきゅうりなどおつまみにも交換できます。丸しぼりみそ赤（100円またはコイン1枚）"},"13":{"comment":"唎酒番所で飲める日本酒やおつまみのみそは館内でも購入できるので、お土産選びにも一役買いそうです。"},"14":{"comment":"「ぽんしゅ館 コンプレックス」には米菓やお米など定番からローカル商品まで厳選された新潟のお土産が揃います。"},"15":{"comment":"飲食スペースもあり、気軽に日本酒を味わえる「角打ち」では、新潟の旬を感じるおつまみと日本酒を楽しめます。"},"16":{"comment":"「爆弾おにぎり家」で味わえる南魚沼産のコシヒカリを使ったおにぎりは絶品。"},"17":{"comment":"ご飯茶碗1杯分の三角形のおにぎりもありますが、今回はその場で握ってもらえる名物「爆弾おにぎり」を注文しました。大きさは二種類。1合分のサイズ（550円～）もありますが今回は4合分の大爆弾おにぎり（2,000円）をチョイス。神楽南蛮味噌や藤五郎梅、佐渡黒豚そぼろなど新潟県産をはじめとした13種類の具材から5種類選べます。"},"18":{"comment":"全型を4枚つないだ大きな海苔を巻いて完成！お米は羽釜で炊いているので柔らかく冷めても美味しいのが特徴。中には、具がぎっしり入っています。"},"19":{"comment":"「私の顔ぐらいある！食べきれるかな、、、？」"},"20":{"comment":"パクッ！"},"21":{"comment":"さすがに全部は食べきれませんでした（笑）。持ち帰りもできるのでぜひ挑戦してみてくださいね！"},"22":{"comment":"「ぽんしゅ館 クラフトマンシップ」では金物の町・燕三条の包丁、カトラリーや、"},"23":{"comment":"県内で作られたぐい呑み、タンブラーなど地域の技を実感できる工芸品が揃います。ぽんしゅ館限定の商品もあるので、お気に入りの一品を探してもよさそうです。"},"24":{"comment":"木村さんもお土産をゲット！ぽんしゅ館で過ごす時間は、旅の思い出をより一層深いものにしてくれるはずです。"},"25":{"comment":"500円で最大5杯分の利酒を楽しめる人気の飲酒スポット。"},"26":{"comment":"好みの一本を見つけたら、お酒売り場で購入も可能。"},"27":{"comment":"新潟のおつまみも品ぞろえが豊富。"},"28":{"comment":"唎酒で目指せ全蔵制覇！"},"29":{"comment":"県内全蔵、111種の地酒を飲み比べられる「唎酒マシン」は圧巻。希少種を飲酒できる「ファーストクラスの唎酒もあり」"},"30":{"comment":"エキナカで利き酒が楽しめる"},"31":{"comment":"県内全蔵の利き酒ができる酒のテーマパーク"},"32":{"comment":"500円でコイン５枚を購入して、おちょこで5銘柄を飲み比べできる。"},"33":{"comment":"100種類以上あるので、お気に入りの銘柄を見つけよう。"},"34":{"comment":"越後の酒蔵の銘柄がずらりと並ぶ利き酒番所"},"35":{"comment":"利き酒番所は酒売り場の奥に。20歳未満は入れない大人のスペース。"},"36":{"comment":"飲みたいお酒を見つけたらコインを入れ、ボタンを押せばOK"},"37":{"comment":"併設させた売店には、越後魚沼が誇る食品や酒器などのグッズが満載！スタッフは利き酒師や清酒達人検定合格者なので、みやげ選びに迷ったら気軽に話しかけてみて。"},"38":{"comment":"ぽんしゅ館の２パターンのロゴをそれぞれあしらったオリジナル升"},"39":{"comment":"好きな銘柄の前掛けを自宅で"},"40":{"comment":"90近い新潟の全酒蔵の代表銘柄が飲める唎き酒コーナーが人気。５００円で５枚の専用コインを購入し、地酒マシンに入れるとお酒を試飲できるので、自分好みのお酒を探すことができます。店内では日本酒だけでなく、お土産も買うことができ、どのお店もJR新幹線校内にあるので便利です。"},"41":{"comment":"新潟県内約90蔵の代表的銘柄から梅酒まで117種もの地酒を利酒できるお酒のテーマパーク。日本酒ビギナーの人でも気軽に楽しめるとあって、県内外のたくさんのお客で賑わっている。"},"42":{"comment":"飲み比べると味の違いがわかって楽しい。"},"43":{"comment":"受付で500円を払い、5枚のコインとおちょこをもらう。１枚で１銘柄のお酒が注がれる。"},"44":{"comment":"冬限定でお燗マシンが登場。お湯の入ったおちょこに数秒浸かせばOK"},"45":{"comment":"新潟の全蔵元を試飲できます。"},"46":{"comment":"新潟の地酒を中心に新潟県全域から集められたお土産を扱う店。"},"47":{"comment":"人気は何といっても全蔵元の味を試し飲みできる利き酒コーナー 。"},"48":{"comment":"500円でおちょこ約５杯分いただける。"},"49":{"comment":"私はここに行きたいがために新潟に行ってると言っても過言でないかもしれません酒ごとに説明も書いてあるので自分の好きなタイプの酒を選ぶことが出来ますあえて好きではないタイプのも挑戦してみたりもしますが、意外と美味しく新発見があったりとても楽しめます会社の同僚が新潟駅に行くので何処かないかと聞かれたので教えてあげたらとても喜んでくれましたお酒好きだけど日本酒苦手って人でも一度行ってみたら新発見があるかもしれません"},"50":{"comment":"新潟のお酒の底力を連想させる豊富なラインナップ。普段はビール党の私が２日で２回訪れました。お土産用にどの銘柄が良いか考えるとすぐ隣に番号順の陳列棚が有りとてもわかりやすかったです"}},"translations":{},"overview":"公開協議の結果、新潟の地元商品とお土産の多様性、地元酒の試飲体験、500円での利き酒体験、爆弾おにぎりと地元のおつまみの評価が高かった。特に、地元の酒蔵からの日本酒の試飲や、地元の食材を使ったおつまみの提供が評価され、これらの体験が観光客に新たな発見を提供しているとの意見が多かった。","config":{"name":"新潟の観光地","question":"ぽんしゅ館の魅力","input":"my-data","model":"gpt-4","extraction":{"workers":11,"limit":50,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\ndef extraction(config):\n    # 出力ディレクトリとファイルパスの設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n\n    # 入力ファイルからコメントデータを読み込む\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    \n    # モデル、プロンプト、ワーカー数、制限数の設定を取得\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    # コメントIDを取得し、コメントIDをインデックスに設定\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    # コメントをバッチに分割して処理\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        \n        # バッチごとの結果をDataFrameに追加\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \n                           \"categoryLabel\": comments.loc[comment_id]['labels'],\n                           \"kutikomi_unique\":comments.loc[comment_id]['kutikomi_unique'],\n                           \"koushiki_unique\":comments.loc[comment_id]['koushiki_unique'],\n                           \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    \n    # 結果をCSVファイルに保存\n    results.to_csv(path, index=False)\n\ndef extract_batch(batch, prompt, model, workers):\n    # スレッドプールを使用してバッチ処理\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\ndef extract_arguments(input, prompt, model, retries=3):\n    # LLM（大規模言語モデル）を使用して引数を抽出\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        # レスポンスをJSONとしてパース\n        obj = json.loads(response)\n        # 文字列の場合、リストに変換\n        if isinstance(obj, str):\n            obj = [obj]\n        # 空文字列を除外\n        items = [a.strip() for a in obj]\n        items = filter(None, items)\n        return items\n    except json.decoder.JSONDecodeError as e:\n        # JSONパースエラー時の処理\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\n\nあなたはプロのリサーチ・アシスタントで、私の仕事を手伝うことがあなたの仕事です。\n私の仕事は、論点を整理したきれいなデータセットを作成することです。\n\nこれから与える投稿をより簡潔で読みやすい意見にするのを手伝ってほしい。\n本当に必要な場合は、2つ以上の別々の意見に分けることもできるが、1つのトピックを返すのが最善であることが多いだろう。\n要約が難しい場合は、そのままの文章を返してください。\n以下にポストを要約する際の事例を挙げます。\nこれらはあくまで文脈の切り離された例であり、この例で与えた文章を返すことはしないでください。\n\n結果は、きちんとフォーマットされた文字列形式（strings）のJSONリストとして返してください。\n\n\n/human\n\n気候変動を考慮したさらなる風水害対策の強化について、都の具体的な計画をお伺いします。\n\n/ai \n\n[\n  \"気候変動を考慮したさらなる風水害対策の強化してほしい。\"\n]\n\n/human \n\n豪雨対策全般の基本方針の検討を進める中、具体的にどのような施策を作ってほしい。\n\n/ai \n\n[\n  \"豪雨対策全般の基本方針の具体的な施策を作ってほしい。\",\n]\n\n/human\n\nAI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\n\n/ai\n\n[\n  \"私たちは、AI技術が環境に与える影響の軽減に焦点を当てるべきである\"\n]\n\n\n/human\n\nいい\n\n/ai\n\n[\n  \"いい\"\n]\n\n/human\n\nあとで読む\n\n/ai\n\n[\n  \"あとで読む\"\n]\n\n/human\n\n読んだ\n\n/ai\n\n[\n  \"読んだ\"\n]\n\n/human\n\n期待\n\n/ai\n\n[\n  \"期待\"\n]","model":"gpt-4"},"clustering":{"clusters":10,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4, or use existing clusters if available.\"\"\"\n\n# 必要なライブラリをインポート\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\ndef clustering(config):\n    # データセットのパスを設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    \n    # 引数のデータを読み込む\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    # 'clusters'カラムが存在するかチェック\n    if 'clusters' in arguments_df.columns:\n        print(\"Using existing cluster labels from args.csv\")\n        cluster_labels = arguments_df['clusters'].values\n        \n        # 埋め込みデータを読み込む\n        embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n        embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n        \n        # UMAPで次元削減\n        umap_model = import_module('umap').UMAP(random_state=42, n_components=2)\n        umap_embeds = umap_model.fit_transform(embeddings_array)\n        \n        # 結果のデータフレームを作成\n        result = pd.DataFrame({\n            'arg-id': arguments_df[\"arg-id\"].values,\n            'x': umap_embeds[:, 0],\n            'y': umap_embeds[:, 1],\n            'cluster-id': cluster_labels\n        })\n        \n    else:\n        print(\"Performing clustering as no existing cluster labels found\")\n        # 埋め込みデータを読み込む\n        embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n        embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n        \n        # クラスター数を設定\n        clusters = config['clustering']['clusters']\n\n        # クラスタリングを実行し結果を保存\n        result = cluster_embeddings(\n            docs=arguments_array,\n            embeddings=embeddings_array,\n            metadatas={\n                \"arg-id\": arguments_df[\"arg-id\"].values,\n                \"comment-id\": arguments_df[\"comment-id\"].values,\n            },\n            n_topics=clusters,\n        )\n    \n    result.to_csv(path, index=False)\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # 動的にモジュールをインポート（これにより、必要な場合にのみインポートされる）\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module('sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    # UMAPモデルを設定\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    \n    # HDBSCANモデルを設定\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    # ストップワードを設定\n    stop = stopwords.words(\"english\")\n    \n    # ベクトライザーモデルを設定\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    \n    # トピックモデルを設定\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # トピックモデルをフィッティング：戻り値：各ドキュメントのトピック予測確率\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    # サンプル数と近傍数を設定\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    \n    # スペクトラルクラスタリングモデルを設定\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # 修正された近傍数を使用\n        random_state=42\n    )\n    \n    # UMAPで次元削減\n    umap_embeds = umap_model.fit_transform(embeddings)\n    \n    # スペクトラルクラスタリングを適用\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    # クラスタリング結果を取得\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    # 結果のカラム名を小文字に変換\n    result.columns = [c.lower() for c in result.columns]\n    \n    # 必要なカラムを選択し、クラスタIDを追加\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result"},"translation":{"model":"gpt-4","languages":[],"flags":[],"source_code":"import json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\n\ndef translation(config):\n    # 出力ディレクトリと保存先パスを設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    # 言語設定を取得\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # 特殊な処理を減らすために空のファイルを作成\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    # 各種データファイルを読み込む\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    # UIで使用する文言をリスト化\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"代表的なコメント\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    # 各データをリストに追加\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    # configに'name'や'question'があれば追加\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n        \n    # 翻訳プロンプトを読み込む\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    # 指定された言語に対して翻訳を実行\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # 長いテイクアウェイや概要を別途翻訳\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    # 結果をまとめる\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    # 結果をJSONファイルとして保存\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    # 指定された言語に翻訳を実行するヘルパー関数\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    # バッチごとに翻訳を実行するヘルパー関数\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\n[\nあなたはプロの翻訳家です。\n単語や文のリストを受け取ります。\n同じリストを、同じ順番で、{言語}に翻訳して返してください。\nオリジナルのリストと同じ長さの有効なJSON文字列リストを返すようにしてください。\n]"},"output_dir":"my-project","previous":{"name":"新潟の観光地","question":"ぽんしゅ館の魅力","input":"my-data","model":"gpt-4","extraction":{"workers":11,"limit":1001,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\ndef extraction(config):\n    # 出力ディレクトリとファイルパスの設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n\n    # 入力ファイルからコメントデータを読み込む\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    \n    # モデル、プロンプト、ワーカー数、制限数の設定を取得\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    # コメントIDを取得し、コメントIDをインデックスに設定\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    # コメントをバッチに分割して処理\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        \n        # バッチごとの結果をDataFrameに追加\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \n                           \"categoryLabel\": comments.loc[comment_id]['labels'],\n                           \"kutikomi_unique\":comments.loc[comment_id]['kutikomi_unique'],\n                           \"koushiki_unique\":comments.loc[comment_id]['koushiki_unique'],\n                           \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    \n    # 結果をCSVファイルに保存\n    results.to_csv(path, index=False)\n\ndef extract_batch(batch, prompt, model, workers):\n    # スレッドプールを使用してバッチ処理\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\ndef extract_arguments(input, prompt, model, retries=3):\n    # LLM（大規模言語モデル）を使用して引数を抽出\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        # レスポンスをJSONとしてパース\n        obj = json.loads(response)\n        # 文字列の場合、リストに変換\n        if isinstance(obj, str):\n            obj = [obj]\n        # 空文字列を除外\n        items = [a.strip() for a in obj]\n        items = filter(None, items)\n        return items\n    except json.decoder.JSONDecodeError as e:\n        # JSONパースエラー時の処理\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\n\nあなたはプロのリサーチ・アシスタントで、私の仕事を手伝うことがあなたの仕事です。\n私の仕事は、論点を整理したきれいなデータセットを作成することです。\n\nこれから与える投稿をより簡潔で読みやすい意見にするのを手伝ってほしい。\n本当に必要な場合は、2つ以上の別々の意見に分けることもできるが、1つのトピックを返すのが最善であることが多いだろう。\n要約が難しい場合は、そのままの文章を返してください。\n以下にポストを要約する際の事例を挙げます。\nこれらはあくまで文脈の切り離された例であり、この例で与えた文章を返すことはしないでください。\n\n結果は、きちんとフォーマットされた文字列形式（strings）のJSONリストとして返してください。\n\n\n/human\n\n気候変動を考慮したさらなる風水害対策の強化について、都の具体的な計画をお伺いします。\n\n/ai \n\n[\n  \"気候変動を考慮したさらなる風水害対策の強化してほしい。\"\n]\n\n/human \n\n豪雨対策全般の基本方針の検討を進める中、具体的にどのような施策を作ってほしい。\n\n/ai \n\n[\n  \"豪雨対策全般の基本方針の具体的な施策を作ってほしい。\",\n]\n\n/human\n\nAI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\n\n/ai\n\n[\n  \"私たちは、AI技術が環境に与える影響の軽減に焦点を当てるべきである\"\n]\n\n\n/human\n\nいい\n\n/ai\n\n[\n  \"いい\"\n]\n\n/human\n\nあとで読む\n\n/ai\n\n[\n  \"あとで読む\"\n]\n\n/human\n\n読んだ\n\n/ai\n\n[\n  \"読んだ\"\n]\n\n/human\n\n期待\n\n/ai\n\n[\n  \"期待\"\n]","model":"gpt-4"},"clustering":{"clusters":10,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\n# 必要なライブラリをインポート\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\ndef clustering(config):\n    # データセットのパスを設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    \n    # 引数のデータを読み込む\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    # 埋め込みデータを読み込む\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    \n    # クラスター数を設定\n    clusters = config['clustering']['clusters']\n\n    # クラスタリングを実行し結果を保存\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # 動的にモジュールをインポート（これにより、必要な場合にのみインポートされる）\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module('sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    # UMAPモデルを設定\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    \n    # HDBSCANモデルを設定\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    # ストップワードを設定\n    stop = stopwords.words(\"english\")\n    \n    # ベクトライザーモデルを設定\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    \n    # トピックモデルを設定\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # トピックモデルをフィッティング：戻り値：各ドキュメントのトピック予測確率\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    # サンプル数と近傍数を設定\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    \n    # スペクトラルクラスタリングモデルを設定\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # 修正された近傍数を使用\n        random_state=42\n    )\n    \n    # UMAPで次元削減\n    # ref:https://mathwords.net/fittransform\n    umap_embeds = umap_model.fit_transform(embeddings)\n    \n    # スペクトラルクラスタリングを適用\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    # クラスタリング結果を取得\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    # 結果のカラム名を小文字に変換\n    result.columns = [c.lower() for c in result.columns]\n    \n    # 必要なカラムを選択し、クラスタIDを追加\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"},"translation":{"model":"gpt-4","languages":[],"flags":[],"source_code":"import json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\n\ndef translation(config):\n    # 出力ディレクトリと保存先パスを設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    # 言語設定を取得\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # 特殊な処理を減らすために空のファイルを作成\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    # 各種データファイルを読み込む\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    # UIで使用する文言をリスト化\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"代表的なコメント\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    # 各データをリストに追加\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    # configに'name'や'question'があれば追加\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n        \n    # 翻訳プロンプトを読み込む\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    # 指定された言語に対して翻訳を実行\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # 長いテイクアウェイや概要を別途翻訳\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    # 結果をまとめる\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    # 結果をJSONファイルとして保存\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    # 指定された言語に翻訳を実行するヘルパー関数\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    # バッチごとに翻訳を実行するヘルパー関数\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\n[\nあなたはプロの翻訳家です。\n単語や文のリストを受け取ります。\n同じリストを、同じ順番で、{言語}に翻訳して返してください。\nオリジナルのリストと同じ長さの有効なJSON文字列リストを返すようにしてください。\n]"},"output_dir":"my-project","embedding":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nあなたは、より広範なコンサルテーション内の一連の議論に対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。あなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\n質問からすでに明らかな文脈は含めない（例えば、相談の質問が「フランスでどのような課題に直面しているか」のようなものであれば、クラスターのラベルに「フランスで」と繰り返す必要はない）。\n\nラベルは非常に簡潔でなければならず、クラスターとその外側にある論点を区別するのに十分な正確さでなければならない。\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定の影響は何だと思いますか？\n\n関心のあるクラスター以外の論点の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限された。\n * 英国は、国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を与えた。\n * 環境基準における協力が減少し、気候変動と闘う努力が妨げられた。\n * 相互医療協定の中断により、患者ケアに課題を感じた。\n * Brexit関連の変更により、家族の居住権や市民権の申請が複雑になった。\n * 英国は、共同研究機会の減少により、研究の課題に取り組む世界的な取り組みに支障をきたすことを目の当たりにした。\n * EUの文化助成プログラムからの除外により、創造的なプロジェクトの制限に直面した。\n * 英国は、EUの資金提供の喪失により、慈善活動やコミュニティ支援の後退を目の当たりにした。\n * 消費者保護の弱体化により、国境を越えた紛争解決に課題が生じた。\n * 英国はプロの音楽家としてEU諸国をツアーする際の制限に直面し、キャリアに影響を与えた。\n\nクラスター内部での議論の例\n\n * Brexitによりサプライチェーンが混乱し、企業にとってコスト増と納期遅延につながった。\n * ブレグジットのため、市場の変動や投資・退職金の不確実性に直面した。\n * 新たな関税や通関手続きにより、英国は輸出業者として利益率の低下に対処した。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転したため、雇用を失った。\n * 英国は輸入品価格の高騰による生活費の増加に苦しんだ。\n * 英国のハイテク産業への投資が減少し、技術革新と雇用機会に影響を与えた。\n * 新たなビザ規制による観光客の減少を目の当たりにし、接客業に影響。\n * ポンド価値の下落により購買力が低下し、旅費が増加した。\n\n\n/ai \n\n財務上のマイナス影響","model":"gpt-4"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system\n\nあなたはシンクタンクで働く専門の研究助手です。公共の協議の際に参加者が述べた意見のリストが提供されます。あなたはそれに基づき、主な見解を1〜2段落で要約して答えます。非常に簡潔で読みやすい短い文章を書きます。\n\n/human\n\n[\n\"銃暴力は私たちの社会において深刻な公衆衛生の危機であると強く信じています。\",\n\"この問題に対処するために包括的な銃規制措置を緊急に講じる必要があります。\",\n\"すべての銃購入者に対する普遍的な背景チェックの実施を支持します。\",\n\"アサルトウェポンと大容量マガジンの禁止に賛成です。\",\n\"違法な銃の密売を防止するための厳格な規制を求めます。\",\n\"銃購入プロセスの一環として精神健康評価を必須にすべきです。\"\n]\n\n/ai\n\n参加者は、普遍的な背景チェック、アサルトウェポン禁止、違法な銃密売の抑制、精神健康評価の優先を強調し、包括的な銃規制を求めました。\n\n\n\n\n\n\n","model":"gpt-4"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. \nOutput should be in Japanese.","model":"gpt-4"},"aggregation":{"source_code":"\"\"\"便利なJSON出力ファイルを生成する\"\"\"\n\nfrom tqdm import tqdm  # プログレスバーを表示するためのライブラリ\nfrom typing import List  # 型ヒントのためのライブラリ\nimport pandas as pd  # データ操作のためのライブラリ\nfrom langchain.chat_models import ChatOpenAI  # 言語モデルを利用するためのライブラリ\nimport json  # JSON操作のためのライブラリ\n\ndef aggregation(config):\n    # 出力ファイルのパスを設定\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    # 結果を格納する辞書を初期化\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    # 引数のCSVファイルを読み込み\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    # コメントのCSVファイルを読み込み\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    \n    # argumentsで参照されているコメントIDのみを抽出し、results辞書に格納\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {\n                'comment': row['comment-body'],\n                #'categoryLabel': row['labels'],\n                #'kutikomi_unique':row['kutikomi_unique']\n                }\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    # オブション：翻訳の設定を取得し、翻訳結果を読み込み\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    # クラスター、ラベル、テイクアウェイのCSVファイルを読み込み\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    # 概要テキストを読み込み\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    # ラベルごとにクラスターを処理し、結果に追加\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            categoryLabel = arguments.loc[arg_id]['categoryLabel'] #追加\n            kutikomi_unique = arguments.loc[arg_id]['kutikomi_unique']\n            koushiki_unique = arguments.loc[arg_id]['koushiki_unique']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'categoryLabel': int(categoryLabel),\n                'kutikomi_unique': int(kutikomi_unique),\n                'koushiki_unique': int(koushiki_unique),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    # 結果をJSONファイルに書き込み\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess #　シェルコマンドを実行するためのモジュール #外部プログラムの起動や、その出力・エラーメッセージの取得を行える\n \n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\" # nextのビルド　#シェルコマンド\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)#シェルコマンドの実施\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"some parameters changed: limit"},{"step":"embedding","run":true,"reason":"some dependent steps will re-run: extraction"},{"step":"clustering","run":true,"reason":"some dependent steps will re-run: embedding"},{"step":"labelling","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"takeaways","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"overview","run":true,"reason":"some dependent steps will re-run: labelling, takeaways"},{"step":"translation","run":true,"reason":"some dependent steps will re-run: extraction, labelling, takeaways, overview"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: extraction, clustering, labelling, takeaways, overview, translation"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"completed","start_time":"2024-07-30T13:32:06.323838","completed_jobs":[{"step":"extraction","completed":"2024-07-30T13:37:13.528490","duration":307.201071,"params":{"workers":11,"limit":1001,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\ndef extraction(config):\n    # 出力ディレクトリとファイルパスの設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n\n    # 入力ファイルからコメントデータを読み込む\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    \n    # モデル、プロンプト、ワーカー数、制限数の設定を取得\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    # コメントIDを取得し、コメントIDをインデックスに設定\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    # コメントをバッチに分割して処理\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        \n        # バッチごとの結果をDataFrameに追加\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \n                           \"categoryLabel\": comments.loc[comment_id]['labels'],\n                           \"kutikomi_unique\":comments.loc[comment_id]['kutikomi_unique'],\n                           \"koushiki_unique\":comments.loc[comment_id]['koushiki_unique'],\n                           \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    \n    # 結果をCSVファイルに保存\n    results.to_csv(path, index=False)\n\ndef extract_batch(batch, prompt, model, workers):\n    # スレッドプールを使用してバッチ処理\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\ndef extract_arguments(input, prompt, model, retries=3):\n    # LLM（大規模言語モデル）を使用して引数を抽出\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        # レスポンスをJSONとしてパース\n        obj = json.loads(response)\n        # 文字列の場合、リストに変換\n        if isinstance(obj, str):\n            obj = [obj]\n        # 空文字列を除外\n        items = [a.strip() for a in obj]\n        items = filter(None, items)\n        return items\n    except json.decoder.JSONDecodeError as e:\n        # JSONパースエラー時の処理\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\n\nあなたはプロのリサーチ・アシスタントで、私の仕事を手伝うことがあなたの仕事です。\n私の仕事は、論点を整理したきれいなデータセットを作成することです。\n\nこれから与える投稿をより簡潔で読みやすい意見にするのを手伝ってほしい。\n本当に必要な場合は、2つ以上の別々の意見に分けることもできるが、1つのトピックを返すのが最善であることが多いだろう。\n要約が難しい場合は、そのままの文章を返してください。\n以下にポストを要約する際の事例を挙げます。\nこれらはあくまで文脈の切り離された例であり、この例で与えた文章を返すことはしないでください。\n\n結果は、きちんとフォーマットされた文字列形式（strings）のJSONリストとして返してください。\n\n\n/human\n\n気候変動を考慮したさらなる風水害対策の強化について、都の具体的な計画をお伺いします。\n\n/ai \n\n[\n  \"気候変動を考慮したさらなる風水害対策の強化してほしい。\"\n]\n\n/human \n\n豪雨対策全般の基本方針の検討を進める中、具体的にどのような施策を作ってほしい。\n\n/ai \n\n[\n  \"豪雨対策全般の基本方針の具体的な施策を作ってほしい。\",\n]\n\n/human\n\nAI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\n\n/ai\n\n[\n  \"私たちは、AI技術が環境に与える影響の軽減に焦点を当てるべきである\"\n]\n\n\n/human\n\nいい\n\n/ai\n\n[\n  \"いい\"\n]\n\n/human\n\nあとで読む\n\n/ai\n\n[\n  \"あとで読む\"\n]\n\n/human\n\n読んだ\n\n/ai\n\n[\n  \"読んだ\"\n]\n\n/human\n\n期待\n\n/ai\n\n[\n  \"期待\"\n]","model":"gpt-4"}},{"step":"embedding","completed":"2024-07-30T13:37:15.780813","duration":2.250312,"params":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}},{"step":"clustering","completed":"2024-07-30T13:37:24.309082","duration":8.527497,"params":{"clusters":10,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\n# 必要なライブラリをインポート\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\ndef clustering(config):\n    # データセットのパスを設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    \n    # 引数のデータを読み込む\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    # 埋め込みデータを読み込む\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    \n    # クラスター数を設定\n    clusters = config['clustering']['clusters']\n\n    # クラスタリングを実行し結果を保存\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # 動的にモジュールをインポート（これにより、必要な場合にのみインポートされる）\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module('sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    # UMAPモデルを設定\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    \n    # HDBSCANモデルを設定\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    # ストップワードを設定\n    stop = stopwords.words(\"english\")\n    \n    # ベクトライザーモデルを設定\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    \n    # トピックモデルを設定\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # トピックモデルをフィッティング：戻り値：各ドキュメントのトピック予測確率\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    # サンプル数と近傍数を設定\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    \n    # スペクトラルクラスタリングモデルを設定\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # 修正された近傍数を使用\n        random_state=42\n    )\n    \n    # UMAPで次元削減\n    # ref:https://mathwords.net/fittransform\n    umap_embeds = umap_model.fit_transform(embeddings)\n    \n    # スペクトラルクラスタリングを適用\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    # クラスタリング結果を取得\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    # 結果のカラム名を小文字に変換\n    result.columns = [c.lower() for c in result.columns]\n    \n    # 必要なカラムを選択し、クラスタIDを追加\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"}},{"step":"labelling","completed":"2024-07-30T13:37:42.968255","duration":18.658443,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nあなたは、より広範なコンサルテーション内の一連の議論に対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。あなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\n質問からすでに明らかな文脈は含めない（例えば、相談の質問が「フランスでどのような課題に直面しているか」のようなものであれば、クラスターのラベルに「フランスで」と繰り返す必要はない）。\n\nラベルは非常に簡潔でなければならず、クラスターとその外側にある論点を区別するのに十分な正確さでなければならない。\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定の影響は何だと思いますか？\n\n関心のあるクラスター以外の論点の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限された。\n * 英国は、国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を与えた。\n * 環境基準における協力が減少し、気候変動と闘う努力が妨げられた。\n * 相互医療協定の中断により、患者ケアに課題を感じた。\n * Brexit関連の変更により、家族の居住権や市民権の申請が複雑になった。\n * 英国は、共同研究機会の減少により、研究の課題に取り組む世界的な取り組みに支障をきたすことを目の当たりにした。\n * EUの文化助成プログラムからの除外により、創造的なプロジェクトの制限に直面した。\n * 英国は、EUの資金提供の喪失により、慈善活動やコミュニティ支援の後退を目の当たりにした。\n * 消費者保護の弱体化により、国境を越えた紛争解決に課題が生じた。\n * 英国はプロの音楽家としてEU諸国をツアーする際の制限に直面し、キャリアに影響を与えた。\n\nクラスター内部での議論の例\n\n * Brexitによりサプライチェーンが混乱し、企業にとってコスト増と納期遅延につながった。\n * ブレグジットのため、市場の変動や投資・退職金の不確実性に直面した。\n * 新たな関税や通関手続きにより、英国は輸出業者として利益率の低下に対処した。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転したため、雇用を失った。\n * 英国は輸入品価格の高騰による生活費の増加に苦しんだ。\n * 英国のハイテク産業への投資が減少し、技術革新と雇用機会に影響を与えた。\n * 新たなビザ規制による観光客の減少を目の当たりにし、接客業に影響。\n * ポンド価値の下落により購買力が低下し、旅費が増加した。\n\n\n/ai \n\n財務上のマイナス影響","model":"gpt-4"}},{"step":"takeaways","completed":"2024-07-30T13:39:12.968807","duration":89.997014,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system\n\nあなたはシンクタンクで働く専門の研究助手です。公共の協議の際に参加者が述べた意見のリストが提供されます。あなたはそれに基づき、主な見解を1〜2段落で要約して答えます。非常に簡潔で読みやすい短い文章を書きます。\n\n/human\n\n[\n\"銃暴力は私たちの社会において深刻な公衆衛生の危機であると強く信じています。\",\n\"この問題に対処するために包括的な銃規制措置を緊急に講じる必要があります。\",\n\"すべての銃購入者に対する普遍的な背景チェックの実施を支持します。\",\n\"アサルトウェポンと大容量マガジンの禁止に賛成です。\",\n\"違法な銃の密売を防止するための厳格な規制を求めます。\",\n\"銃購入プロセスの一環として精神健康評価を必須にすべきです。\"\n]\n\n/ai\n\n参加者は、普遍的な背景チェック、アサルトウェポン禁止、違法な銃密売の抑制、精神健康評価の優先を強調し、包括的な銃規制を求めました。\n\n\n\n\n\n\n","model":"gpt-4"}},{"step":"overview","completed":"2024-07-30T13:39:21.606882","duration":8.634334,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. \nOutput should be in Japanese.","model":"gpt-4"}},{"step":"translation","completed":"2024-07-30T13:39:21.615822","duration":0.004497,"params":{"model":"gpt-4","languages":[],"flags":[],"source_code":"import json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\n\ndef translation(config):\n    # 出力ディレクトリと保存先パスを設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    # 言語設定を取得\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # 特殊な処理を減らすために空のファイルを作成\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    # 各種データファイルを読み込む\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    # UIで使用する文言をリスト化\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"代表的なコメント\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    # 各データをリストに追加\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    # configに'name'や'question'があれば追加\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n        \n    # 翻訳プロンプトを読み込む\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    # 指定された言語に対して翻訳を実行\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # 長いテイクアウェイや概要を別途翻訳\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    # 結果をまとめる\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    # 結果をJSONファイルとして保存\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    # 指定された言語に翻訳を実行するヘルパー関数\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    # バッチごとに翻訳を実行するヘルパー関数\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\n[\nあなたはプロの翻訳家です。\n単語や文のリストを受け取ります。\n同じリストを、同じ順番で、{言語}に翻訳して返してください。\nオリジナルのリストと同じ長さの有効なJSON文字列リストを返すようにしてください。\n]"}},{"step":"aggregation","completed":"2024-07-30T13:39:21.724879","duration":0.106263,"params":{"source_code":"\"\"\"便利なJSON出力ファイルを生成する\"\"\"\n\nfrom tqdm import tqdm  # プログレスバーを表示するためのライブラリ\nfrom typing import List  # 型ヒントのためのライブラリ\nimport pandas as pd  # データ操作のためのライブラリ\nfrom langchain.chat_models import ChatOpenAI  # 言語モデルを利用するためのライブラリ\nimport json  # JSON操作のためのライブラリ\n\ndef aggregation(config):\n    # 出力ファイルのパスを設定\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    # 結果を格納する辞書を初期化\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    # 引数のCSVファイルを読み込み\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    # コメントのCSVファイルを読み込み\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    \n    # argumentsで参照されているコメントIDのみを抽出し、results辞書に格納\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {\n                'comment': row['comment-body'],\n                #'categoryLabel': row['labels'],\n                #'kutikomi_unique':row['kutikomi_unique']\n                }\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    # オブション：翻訳の設定を取得し、翻訳結果を読み込み\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    # クラスター、ラベル、テイクアウェイのCSVファイルを読み込み\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    # 概要テキストを読み込み\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    # ラベルごとにクラスターを処理し、結果に追加\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            categoryLabel = arguments.loc[arg_id]['categoryLabel'] #追加\n            kutikomi_unique = arguments.loc[arg_id]['kutikomi_unique']\n            koushiki_unique = arguments.loc[arg_id]['koushiki_unique']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'categoryLabel': int(categoryLabel),\n                'kutikomi_unique': int(kutikomi_unique),\n                'koushiki_unique': int(koushiki_unique),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    # 結果をJSONファイルに書き込み\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"}},{"step":"visualization","completed":"2024-07-30T13:39:26.317380","duration":4.591841,"params":{"replacements":[],"source_code":"\nimport subprocess #　シェルコマンドを実行するためのモジュール #外部プログラムの起動や、その出力・エラーメッセージの取得を行える\n \n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\" # nextのビルド　#シェルコマンド\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)#シェルコマンドの実施\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"}}],"lock_until":"2024-07-30T13:44:26.318447","current_job":"visualization","current_job_started":"2024-07-30T13:39:21.725567","previously_completed_jobs":[],"end_time":"2024-07-30T13:39:26.318442"},"embedding":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nあなたは、より広範なコンサルテーション内の一連の議論に対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。あなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\n質問からすでに明らかな文脈は含めない（例えば、相談の質問が「フランスでどのような課題に直面しているか」のようなものであれば、クラスターのラベルに「フランスで」と繰り返す必要はない）。\n\nラベルは非常に簡潔でなければならず、クラスターとその外側にある論点を区別するのに十分な正確さでなければならない。\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定の影響は何だと思いますか？\n\n関心のあるクラスター以外の論点の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限された。\n * 英国は、国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を与えた。\n * 環境基準における協力が減少し、気候変動と闘う努力が妨げられた。\n * 相互医療協定の中断により、患者ケアに課題を感じた。\n * Brexit関連の変更により、家族の居住権や市民権の申請が複雑になった。\n * 英国は、共同研究機会の減少により、研究の課題に取り組む世界的な取り組みに支障をきたすことを目の当たりにした。\n * EUの文化助成プログラムからの除外により、創造的なプロジェクトの制限に直面した。\n * 英国は、EUの資金提供の喪失により、慈善活動やコミュニティ支援の後退を目の当たりにした。\n * 消費者保護の弱体化により、国境を越えた紛争解決に課題が生じた。\n * 英国はプロの音楽家としてEU諸国をツアーする際の制限に直面し、キャリアに影響を与えた。\n\nクラスター内部での議論の例\n\n * Brexitによりサプライチェーンが混乱し、企業にとってコスト増と納期遅延につながった。\n * ブレグジットのため、市場の変動や投資・退職金の不確実性に直面した。\n * 新たな関税や通関手続きにより、英国は輸出業者として利益率の低下に対処した。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転したため、雇用を失った。\n * 英国は輸入品価格の高騰による生活費の増加に苦しんだ。\n * 英国のハイテク産業への投資が減少し、技術革新と雇用機会に影響を与えた。\n * 新たなビザ規制による観光客の減少を目の当たりにし、接客業に影響。\n * ポンド価値の下落により購買力が低下し、旅費が増加した。\n\n\n/ai \n\n財務上のマイナス影響","model":"gpt-4"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system\n\nあなたはシンクタンクで働く専門の研究助手です。公共の協議の際に参加者が述べた意見のリストが提供されます。あなたはそれに基づき、主な見解を1〜2段落で要約して答えます。非常に簡潔で読みやすい短い文章を書きます。\n\n/human\n\n[\n\"銃暴力は私たちの社会において深刻な公衆衛生の危機であると強く信じています。\",\n\"この問題に対処するために包括的な銃規制措置を緊急に講じる必要があります。\",\n\"すべての銃購入者に対する普遍的な背景チェックの実施を支持します。\",\n\"アサルトウェポンと大容量マガジンの禁止に賛成です。\",\n\"違法な銃の密売を防止するための厳格な規制を求めます。\",\n\"銃購入プロセスの一環として精神健康評価を必須にすべきです。\"\n]\n\n/ai\n\n参加者は、普遍的な背景チェック、アサルトウェポン禁止、違法な銃密売の抑制、精神健康評価の優先を強調し、包括的な銃規制を求めました。\n\n\n\n\n\n\n","model":"gpt-4"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. \nOutput should be in Japanese.","model":"gpt-4"},"aggregation":{"source_code":"\"\"\"便利なJSON出力ファイルを生成する\"\"\"\n\nfrom tqdm import tqdm  # プログレスバーを表示するためのライブラリ\nfrom typing import List  # 型ヒントのためのライブラリ\nimport pandas as pd  # データ操作のためのライブラリ\nfrom langchain.chat_models import ChatOpenAI  # 言語モデルを利用するためのライブラリ\nimport json  # JSON操作のためのライブラリ\n\ndef aggregation(config):\n    # 出力ファイルのパスを設定\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    # 結果を格納する辞書を初期化\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    # 引数のCSVファイルを読み込み\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    # コメントのCSVファイルを読み込み\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    \n    # argumentsで参照されているコメントIDのみを抽出し、results辞書に格納\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {\n                'comment': row['comment-body'],\n                #'categoryLabel': row['labels'],\n                #'kutikomi_unique':row['kutikomi_unique']\n                }\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    # オブション：翻訳の設定を取得し、翻訳結果を読み込み\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    # クラスター、ラベル、テイクアウェイのCSVファイルを読み込み\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    # 概要テキストを読み込み\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    # ラベルごとにクラスターを処理し、結果に追加\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            categoryLabel = arguments.loc[arg_id]['categoryLabel'] #追加\n            kutikomi_unique = arguments.loc[arg_id]['kutikomi_unique']\n            koushiki_unique = arguments.loc[arg_id]['koushiki_unique']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'categoryLabel': int(categoryLabel),\n                'kutikomi_unique': int(kutikomi_unique),\n                'koushiki_unique': int(koushiki_unique),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    # 結果をJSONファイルに書き込み\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess #　シェルコマンドを実行するためのモジュール #外部プログラムの起動や、その出力・エラーメッセージの取得を行える\n \n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\" # nextのビルド　#シェルコマンド\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)#シェルコマンドの実施\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"some parameters changed: limit"},{"step":"embedding","run":true,"reason":"some dependent steps will re-run: extraction"},{"step":"clustering","run":true,"reason":"some dependent steps will re-run: embedding"},{"step":"labelling","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"takeaways","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"overview","run":true,"reason":"some dependent steps will re-run: labelling, takeaways"},{"step":"translation","run":true,"reason":"some dependent steps will re-run: extraction, labelling, takeaways, overview"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: extraction, clustering, labelling, takeaways, overview, translation"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"running","start_time":"2024-07-30T14:24:08.863548","completed_jobs":[{"step":"extraction","completed":"2024-07-30T14:24:31.381503","duration":22.514987,"params":{"workers":11,"limit":50,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\ndef extraction(config):\n    # 出力ディレクトリとファイルパスの設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n\n    # 入力ファイルからコメントデータを読み込む\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    \n    # モデル、プロンプト、ワーカー数、制限数の設定を取得\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    # コメントIDを取得し、コメントIDをインデックスに設定\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    # コメントをバッチに分割して処理\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        \n        # バッチごとの結果をDataFrameに追加\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \n                           \"categoryLabel\": comments.loc[comment_id]['labels'],\n                           \"kutikomi_unique\":comments.loc[comment_id]['kutikomi_unique'],\n                           \"koushiki_unique\":comments.loc[comment_id]['koushiki_unique'],\n                           \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    \n    # 結果をCSVファイルに保存\n    results.to_csv(path, index=False)\n\ndef extract_batch(batch, prompt, model, workers):\n    # スレッドプールを使用してバッチ処理\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\ndef extract_arguments(input, prompt, model, retries=3):\n    # LLM（大規模言語モデル）を使用して引数を抽出\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        # レスポンスをJSONとしてパース\n        obj = json.loads(response)\n        # 文字列の場合、リストに変換\n        if isinstance(obj, str):\n            obj = [obj]\n        # 空文字列を除外\n        items = [a.strip() for a in obj]\n        items = filter(None, items)\n        return items\n    except json.decoder.JSONDecodeError as e:\n        # JSONパースエラー時の処理\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\n\nあなたはプロのリサーチ・アシスタントで、私の仕事を手伝うことがあなたの仕事です。\n私の仕事は、論点を整理したきれいなデータセットを作成することです。\n\nこれから与える投稿をより簡潔で読みやすい意見にするのを手伝ってほしい。\n本当に必要な場合は、2つ以上の別々の意見に分けることもできるが、1つのトピックを返すのが最善であることが多いだろう。\n要約が難しい場合は、そのままの文章を返してください。\n以下にポストを要約する際の事例を挙げます。\nこれらはあくまで文脈の切り離された例であり、この例で与えた文章を返すことはしないでください。\n\n結果は、きちんとフォーマットされた文字列形式（strings）のJSONリストとして返してください。\n\n\n/human\n\n気候変動を考慮したさらなる風水害対策の強化について、都の具体的な計画をお伺いします。\n\n/ai \n\n[\n  \"気候変動を考慮したさらなる風水害対策の強化してほしい。\"\n]\n\n/human \n\n豪雨対策全般の基本方針の検討を進める中、具体的にどのような施策を作ってほしい。\n\n/ai \n\n[\n  \"豪雨対策全般の基本方針の具体的な施策を作ってほしい。\",\n]\n\n/human\n\nAI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\n\n/ai\n\n[\n  \"私たちは、AI技術が環境に与える影響の軽減に焦点を当てるべきである\"\n]\n\n\n/human\n\nいい\n\n/ai\n\n[\n  \"いい\"\n]\n\n/human\n\nあとで読む\n\n/ai\n\n[\n  \"あとで読む\"\n]\n\n/human\n\n読んだ\n\n/ai\n\n[\n  \"読んだ\"\n]\n\n/human\n\n期待\n\n/ai\n\n[\n  \"期待\"\n]","model":"gpt-4"}},{"step":"embedding","completed":"2024-07-30T14:24:32.720284","duration":1.337129,"params":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}},{"step":"clustering","completed":"2024-07-30T14:24:45.023823","duration":12.301928,"params":{"clusters":10,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4, or use existing clusters if available.\"\"\"\n\n# 必要なライブラリをインポート\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\ndef clustering(config):\n    # データセットのパスを設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    \n    # 引数のデータを読み込む\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    # 'clusters'カラムが存在するかチェック\n    if 'clusters' in arguments_df.columns:\n        print(\"Using existing cluster labels from args.csv\")\n        cluster_labels = arguments_df['clusters'].values\n        \n        # 埋め込みデータを読み込む\n        embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n        embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n        \n        # UMAPで次元削減\n        umap_model = import_module('umap').UMAP(random_state=42, n_components=2)\n        umap_embeds = umap_model.fit_transform(embeddings_array)\n        \n        # 結果のデータフレームを作成\n        result = pd.DataFrame({\n            'arg-id': arguments_df[\"arg-id\"].values,\n            'x': umap_embeds[:, 0],\n            'y': umap_embeds[:, 1],\n            'cluster-id': cluster_labels\n        })\n        \n    else:\n        print(\"Performing clustering as no existing cluster labels found\")\n        # 埋め込みデータを読み込む\n        embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n        embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n        \n        # クラスター数を設定\n        clusters = config['clustering']['clusters']\n\n        # クラスタリングを実行し結果を保存\n        result = cluster_embeddings(\n            docs=arguments_array,\n            embeddings=embeddings_array,\n            metadatas={\n                \"arg-id\": arguments_df[\"arg-id\"].values,\n                \"comment-id\": arguments_df[\"comment-id\"].values,\n            },\n            n_topics=clusters,\n        )\n    \n    result.to_csv(path, index=False)\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # 動的にモジュールをインポート（これにより、必要な場合にのみインポートされる）\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module('sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    # UMAPモデルを設定\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    \n    # HDBSCANモデルを設定\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    # ストップワードを設定\n    stop = stopwords.words(\"english\")\n    \n    # ベクトライザーモデルを設定\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    \n    # トピックモデルを設定\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # トピックモデルをフィッティング：戻り値：各ドキュメントのトピック予測確率\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    # サンプル数と近傍数を設定\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    \n    # スペクトラルクラスタリングモデルを設定\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # 修正された近傍数を使用\n        random_state=42\n    )\n    \n    # UMAPで次元削減\n    umap_embeds = umap_model.fit_transform(embeddings)\n    \n    # スペクトラルクラスタリングを適用\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    # クラスタリング結果を取得\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    # 結果のカラム名を小文字に変換\n    result.columns = [c.lower() for c in result.columns]\n    \n    # 必要なカラムを選択し、クラスタIDを追加\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result"}},{"step":"labelling","completed":"2024-07-30T14:25:00.145164","duration":15.120572,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nあなたは、より広範なコンサルテーション内の一連の議論に対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。あなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\n質問からすでに明らかな文脈は含めない（例えば、相談の質問が「フランスでどのような課題に直面しているか」のようなものであれば、クラスターのラベルに「フランスで」と繰り返す必要はない）。\n\nラベルは非常に簡潔でなければならず、クラスターとその外側にある論点を区別するのに十分な正確さでなければならない。\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定の影響は何だと思いますか？\n\n関心のあるクラスター以外の論点の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限された。\n * 英国は、国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を与えた。\n * 環境基準における協力が減少し、気候変動と闘う努力が妨げられた。\n * 相互医療協定の中断により、患者ケアに課題を感じた。\n * Brexit関連の変更により、家族の居住権や市民権の申請が複雑になった。\n * 英国は、共同研究機会の減少により、研究の課題に取り組む世界的な取り組みに支障をきたすことを目の当たりにした。\n * EUの文化助成プログラムからの除外により、創造的なプロジェクトの制限に直面した。\n * 英国は、EUの資金提供の喪失により、慈善活動やコミュニティ支援の後退を目の当たりにした。\n * 消費者保護の弱体化により、国境を越えた紛争解決に課題が生じた。\n * 英国はプロの音楽家としてEU諸国をツアーする際の制限に直面し、キャリアに影響を与えた。\n\nクラスター内部での議論の例\n\n * Brexitによりサプライチェーンが混乱し、企業にとってコスト増と納期遅延につながった。\n * ブレグジットのため、市場の変動や投資・退職金の不確実性に直面した。\n * 新たな関税や通関手続きにより、英国は輸出業者として利益率の低下に対処した。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転したため、雇用を失った。\n * 英国は輸入品価格の高騰による生活費の増加に苦しんだ。\n * 英国のハイテク産業への投資が減少し、技術革新と雇用機会に影響を与えた。\n * 新たなビザ規制による観光客の減少を目の当たりにし、接客業に影響。\n * ポンド価値の下落により購買力が低下し、旅費が増加した。\n\n\n/ai \n\n財務上のマイナス影響","model":"gpt-4"}},{"step":"takeaways","completed":"2024-07-30T14:26:16.751988","duration":76.605204,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system\n\nあなたはシンクタンクで働く専門の研究助手です。公共の協議の際に参加者が述べた意見のリストが提供されます。あなたはそれに基づき、主な見解を1〜2段落で要約して答えます。非常に簡潔で読みやすい短い文章を書きます。\n\n/human\n\n[\n\"銃暴力は私たちの社会において深刻な公衆衛生の危機であると強く信じています。\",\n\"この問題に対処するために包括的な銃規制措置を緊急に講じる必要があります。\",\n\"すべての銃購入者に対する普遍的な背景チェックの実施を支持します。\",\n\"アサルトウェポンと大容量マガジンの禁止に賛成です。\",\n\"違法な銃の密売を防止するための厳格な規制を求めます。\",\n\"銃購入プロセスの一環として精神健康評価を必須にすべきです。\"\n]\n\n/ai\n\n参加者は、普遍的な背景チェック、アサルトウェポン禁止、違法な銃密売の抑制、精神健康評価の優先を強調し、包括的な銃規制を求めました。\n\n\n\n\n\n\n","model":"gpt-4"}},{"step":"overview","completed":"2024-07-30T14:26:25.492073","duration":8.736784,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. \nOutput should be in Japanese.","model":"gpt-4"}},{"step":"translation","completed":"2024-07-30T14:26:25.497864","duration":0.002907,"params":{"model":"gpt-4","languages":[],"flags":[],"source_code":"import json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\n\ndef translation(config):\n    # 出力ディレクトリと保存先パスを設定\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    # 言語設定を取得\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # 特殊な処理を減らすために空のファイルを作成\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    # 各種データファイルを読み込む\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    # UIで使用する文言をリスト化\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"代表的なコメント\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    # 各データをリストに追加\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    # configに'name'や'question'があれば追加\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n        \n    # 翻訳プロンプトを読み込む\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    # 指定された言語に対して翻訳を実行\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # 長いテイクアウェイや概要を別途翻訳\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    # 結果をまとめる\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    # 結果をJSONファイルとして保存\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    # 指定された言語に翻訳を実行するヘルパー関数\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    # バッチごとに翻訳を実行するヘルパー関数\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\n[\nあなたはプロの翻訳家です。\n単語や文のリストを受け取ります。\n同じリストを、同じ順番で、{言語}に翻訳して返してください。\nオリジナルのリストと同じ長さの有効なJSON文字列リストを返すようにしてください。\n]"}}],"lock_until":"2024-07-30T14:31:25.500675","current_job":"aggregation","current_job_started":"2024-07-30T14:26:25.500663"}}},"__N_SSG":true}